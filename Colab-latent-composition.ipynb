{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Colab-latent-composition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWNt1kHWt4nO"
      },
      "source": [
        "# Colab-latent-composition \n",
        "Original repo: [chail/latent-composition](https://github.com/chail/latent-composition)\n",
        "\n",
        "Original colab: [here](https://github.com/chail/latent-composition/blob/main/notebooks/quickstart.ipynb)\n",
        "\n",
        "My fork: [Colab-latent-composition](https://github.com/styler00dollar/Colab-latent-composition)\n",
        "\n",
        "Making a more compact and colab-friendly version by using ``@title`` and fixing dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_fnffi-ugnJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sym40qEZt0Bc",
        "cellView": "form"
      },
      "source": [
        "#@title install\n",
        "!git clone https://github.com/chail/latent-composition\n",
        "%cd /content/latent-composition\n",
        "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES=0\n",
        "import torch\n",
        "from networks import networks\n",
        "from PIL import Image\n",
        "from utils import show, renormalize, compositions, masking, imutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install ninja\n",
        "!pip install prdc\n",
        "!pip install lpips\n",
        "%cd /content/\n",
        "!wget https://raw.githubusercontent.com/rosinality/id-gan-pytorch/master/stylegan2/op/fused_bias_act.cpp\n",
        "!wget https://raw.githubusercontent.com/rosinality/id-gan-pytorch/master/stylegan2/op/fused_bias_act_kernel.cu\n",
        "%cd /content/latent-composition\n",
        "!sh resources/download_resources.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "boJX1eAPt0Bl"
      },
      "source": [
        "#@title fused_act.py (fixing module load)\n",
        "%%writefile /content/latent-composition/utils/PT_STYLEGAN2/op/fused_act.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "\n",
        "module_path = os.path.dirname(__file__)\n",
        "\"\"\"\n",
        "fused = load(\n",
        "    name='fused',\n",
        "    sources=[\n",
        "        '/content/fused_bias_act.cpp',\n",
        "        '/content/fused_bias_act_kernel.cu',\n",
        "    ],\n",
        ")\n",
        "\"\"\"\n",
        "#https://github.com/rosinality/id-gan-pytorch/tree/master/stylegan2/op\n",
        "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
        "fused = load(\n",
        "    name='fused_bias_act',\n",
        "    sources=[\n",
        "        '/content/fused_bias_act.cpp',\n",
        "        '/content/fused_bias_act_kernel.cu',\n",
        "    ],\n",
        ")\n",
        "\n",
        "class FusedLeakyReLUFunctionBackward(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, out, negative_slope, scale):\n",
        "        ctx.save_for_backward(out)\n",
        "        ctx.negative_slope = negative_slope\n",
        "        ctx.scale = scale\n",
        "\n",
        "        empty = grad_output.new_empty(0)\n",
        "\n",
        "        grad_input = fused.fused_bias_act(\n",
        "            grad_output, empty, out, 3, 1, negative_slope, scale\n",
        "        )\n",
        "\n",
        "        dim = [0]\n",
        "\n",
        "        if grad_input.ndim > 2:\n",
        "            dim += list(range(2, grad_input.ndim))\n",
        "\n",
        "        grad_bias = grad_input.sum(dim).detach()\n",
        "\n",
        "        return grad_input, grad_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_input, gradgrad_bias):\n",
        "        out, = ctx.saved_tensors\n",
        "        gradgrad_out = fused.fused_bias_act(\n",
        "            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n",
        "        )\n",
        "\n",
        "        return gradgrad_out, None, None, None\n",
        "\n",
        "\n",
        "class FusedLeakyReLUFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, bias, negative_slope, scale):\n",
        "        empty = input.new_empty(0)\n",
        "        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n",
        "        ctx.save_for_backward(out)\n",
        "        ctx.negative_slope = negative_slope\n",
        "        ctx.scale = scale\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        out, = ctx.saved_tensors\n",
        "\n",
        "        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n",
        "            grad_output, out, ctx.negative_slope, ctx.scale\n",
        "        )\n",
        "\n",
        "        return grad_input, grad_bias, None, None\n",
        "\n",
        "\n",
        "class FusedLeakyReLU(nn.Module):\n",
        "    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(channel))\n",
        "        self.negative_slope = negative_slope\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input):\n",
        "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
        "\n",
        "\n",
        "def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n",
        "    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E94KDlQt0Bm",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "from torchvision import transforms\n",
        "def load_nets():\n",
        "    \n",
        "    # bonus stylegan encoder trained on real images + identity loss\n",
        "    # nets = networks.define_nets('stylegan', 'ffhq', ckpt_path='pretrained_models/sgan_encoders/ffhq_reals_RGBM/netE_epoch_best.pth')\n",
        "    \n",
        "    # stylegan trained on gsamples + identity loss\n",
        "    nets = networks.define_nets('stylegan', 'ffhq')\n",
        "\n",
        "    return nets\n",
        "\n",
        "im_path = '/content/0.jpg' #@param {type:\"string\"}\n",
        "\n",
        "outdim=1024 # for faces\n",
        "# outdim = 256 # for churches\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                transforms.Resize(outdim),\n",
        "                transforms.CenterCrop(outdim),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])    \n",
        "source_im = transform(Image.open(im_path).convert('RGB'))[None].cuda()\n",
        "show(['Source Image', renormalize.as_image(source_im[0]).resize((256, 256), Image.LANCZOS)])\n",
        "\n",
        "# need to reload nets each time after finetuning\n",
        "nets = load_nets()\n",
        "outdim = nets.setting['outdim']\n",
        "\n",
        "with torch.no_grad():\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    out = nets.invert(source_im, mask)\n",
        "    # encoded = nets.encode(source_im, mask)\n",
        "    # out = nets.decode(encoded)\n",
        "    show(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGmKTK6pt0Bp",
        "cellView": "form"
      },
      "source": [
        "#@title finetune the encoder towards the real image\n",
        "%cd /content/latent-composition\n",
        "from networks.psp import id_loss\n",
        "from utils import util, losses, pbar\n",
        "\n",
        "# need to reload nets each time after finetuning\n",
        "nets = load_nets()\n",
        "outdim = nets.setting['outdim']\n",
        "\n",
        "with torch.no_grad():\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    initial_inversion = nets.invert(source_im, mask)\n",
        "\n",
        "batch_size = 1\n",
        "lambda_mse = 1.0\n",
        "lambda_lpips = 1.0\n",
        "lambda_z = 0. # set lambda_z to 10.0 to optimize the latent first. (optional)\n",
        "lambda_id = 0.1\n",
        "\n",
        "# do optional latent optimization\n",
        "if lambda_z > 0.:\n",
        "    checkpoint_dict, opt_losses = inversions.invert_lbfgs(nets, source_im, num_steps=30)\n",
        "    opt_ws = checkpoint_dict['current_z'].detach().clone().repeat(batch_size, 1, 1)\n",
        "    # reenable grad after LBFGS\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "netG = nets.generator.eval()\n",
        "netE = nets.encoder.eval()\n",
        "util.set_requires_grad(False, netG)\n",
        "util.set_requires_grad(True, netE)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "l1_loss = torch.nn.L1Loss()\n",
        "perceptual_loss = losses.LPIPS_Loss().cuda().eval()\n",
        "identity_loss = id_loss.IDLoss().cuda().eval()\n",
        "util.set_requires_grad(False, identity_loss)\n",
        "util.set_requires_grad(False, perceptual_loss)\n",
        "\n",
        "optimizer = torch.optim.Adam(netE.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
        "\n",
        "target = source_im.repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "reshape = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
        "\n",
        "all_losses = dict(z=[], mse=[], lpips=[], id=[], sim_improvement=[])\n",
        "\n",
        "# 30-50 steps is about enough\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for i in pbar(range(30)):\n",
        "    optimizer.zero_grad()\n",
        "    mask_data = [masking.mask_upsample(source_im) for _ in range(batch_size)]\n",
        "    hints = torch.cat([m[0] for m in mask_data])\n",
        "    masks = torch.cat([m[1] for m in mask_data])\n",
        "    \n",
        "    encoded = netE(torch.cat([hints, masks], dim=1))\n",
        "    regenerated = netG(encoded)\n",
        "    if lambda_z > 0.:\n",
        "        loss_z = mse_loss(encoded, opt_ws)\n",
        "    else:\n",
        "        loss_z = torch.Tensor((0.,)).cuda()\n",
        "    loss_mse = mse_loss(regenerated, target)\n",
        "    loss_perceptual = perceptual_loss.forward(\n",
        "        reshape(regenerated), reshape(target)).mean()\n",
        "    loss_id, sim_improvement, id_logs = identity_loss(reshape(regenerated), reshape(target), reshape(target))\n",
        "    loss = (lambda_z * loss_z + lambda_mse * loss_mse\n",
        "            + lambda_lpips * loss_perceptual + lambda_id * loss_id)\n",
        "    # loss.backward(retain_graph=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    all_losses['z'].append(loss_z.item())\n",
        "    all_losses['mse'].append(loss_mse.item())\n",
        "    all_losses['lpips'].append(loss_perceptual.item())\n",
        "    all_losses['id'].append(loss_id)\n",
        "    all_losses['sim_improvement'].append(sim_improvement)\n",
        "\n",
        "f, ax = plt.subplots(1,4, figsize=(16, 3))\n",
        "ax[0].plot(all_losses['z'])\n",
        "ax[0].set_title('Z loss')\n",
        "ax[1].plot(all_losses['mse'])\n",
        "ax[1].set_title('MSE loss')\n",
        "ax[2].plot(all_losses['lpips'])\n",
        "ax[2].set_title('LPIPS loss')\n",
        "ax[3].plot(all_losses['id'])\n",
        "ax[3].set_title('ID loss')\n",
        "\n",
        "\n",
        "show.a(['Initial Inversion', renormalize.as_image(initial_inversion[0]).resize((256, 256), Image.LANCZOS)])\n",
        "if lambda_z > 0.:\n",
        "    show.a(['optimized w', renormalize.as_image(checkpoint_dict['current_x'][0]).resize((256, 256), Image.LANCZOS)])\n",
        "show.flush()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    hints = source_im\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    \n",
        "    # hints, mask = masking.mask_upsample(source_im, threshold=0.5) \n",
        "    # mask = mask+0.5\n",
        "    \n",
        "    encoded = nets.encode(hints, mask)\n",
        "    out = nets.decode(encoded)\n",
        "    show.a(['hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.a(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.flush()\n",
        "    \n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    mask[:, :, 100:-100, 100:-100] = 0.\n",
        "    hints = source_im*mask\n",
        "    \n",
        "    encoded = nets.encode(hints, mask)\n",
        "    out = nets.decode(encoded)\n",
        "    show.a(['Hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.a(['Inverted Hints', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SITLeD81t0Bt",
        "cellView": "form"
      },
      "source": [
        "#@title interactive mixing\n",
        "\n",
        "#@markdown Draw your mouse on the image panels. The network input will show in the second to last panel, and the network output in the last panel.\n",
        "\n",
        "#@markdown You can add more images, or just delete paths to use less images.\n",
        "\n",
        "path1 = '/content/0.jpg' #@param {type:\"string\"}\n",
        "path2 = '/content/1.jpg' #@param {type:\"string\"}\n",
        "path3 = '/content/2.webp' #@param {type:\"string\"}\n",
        "path4 = '/content/3.jpg' #@param {type:\"string\"}\n",
        "\n",
        "collage_paths = [\n",
        "    path1,\n",
        "    path2,\n",
        "    path3,\n",
        "    path4\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "num_components = len(collage_paths)\n",
        "collage_ims = torch.cat([transform(Image.open(p).convert('RGB'))[None].cuda() for p in collage_paths])\n",
        "\n",
        "from utils import paintwidget, labwidget\n",
        "\n",
        "def make_callback(painter):\n",
        "    def probe_changed(c):\n",
        "        global composite\n",
        "        global mask_composite\n",
        "        p = painter\n",
        "        if p.mask:\n",
        "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
        "        else:\n",
        "            mask = torch.zeros_like(sample)[None]\n",
        "        with torch.no_grad():\n",
        "            mask = mask[:, [0], :, :].cuda()\n",
        "            mask_composite += mask\n",
        "            sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
        "            \n",
        "            composite = sample * mask + composite * (1-mask)\n",
        "            mask_composite = torch.clamp(mask_composite, 0., 1.)\n",
        "            out = nets.invert(composite, mask_composite)\n",
        "        img_url = renormalize.as_url(composite[0], size=256)\n",
        "        img_html = '<img src=\"%s\"/>'% img_url\n",
        "        collage_div.innerHTML = img_html   \n",
        "        img_url = renormalize.as_url(out[0], size=256)\n",
        "        img_html = '<img src=\"%s\"/>'% img_url\n",
        "        encoded_div.innerHTML = img_html\n",
        "    return probe_changed\n",
        "\n",
        "img_url = renormalize.as_url(torch.zeros(3, outdim, outdim), size=256)\n",
        "img_html = '<img src=\"%s\"/>'%img_url\n",
        "encoded_div = labwidget.Div(img_html)\n",
        "collage_div = labwidget.Div(img_html)\n",
        "\n",
        "painters = []\n",
        "\n",
        "composite = torch.zeros(1, 3, outdim, outdim).cuda()\n",
        "mask_composite = torch.zeros_like(composite)[:, [0], :, :]\n",
        "\n",
        "for i in range(num_components):\n",
        "    src_painter = paintwidget.PaintWidget(oneshot=False, width=256, height=256, \n",
        "                                      brushsize=20, save_sequence=False, track_move=True) # , on_move=True)\n",
        "    src_painter.image = renormalize.as_url(collage_ims[i], size=256)\n",
        "    painters.append(src_painter)\n",
        "    callback = make_callback(src_painter)\n",
        "    src_painter.on('mask', callback)\n",
        "    show.a([src_painter], cols=3)\n",
        "\n",
        "show.a([collage_div], cols=3)\n",
        "show.a([encoded_div], cols=3)\n",
        "show.flush()\n",
        "\n",
        "def show_drawing():\n",
        "    for i, p in enumerate(painters):\n",
        "        if p.mask:\n",
        "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
        "        else:\n",
        "            mask = torch.zeros(1, 3, outdim, outdim).cuda()\n",
        "        mask = mask[:, [0], :, :].cuda()\n",
        "        sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
        "        part = sample * mask\n",
        "        im_pil = imutil.draw_masked_image(sample, mask, size=256)[1]\n",
        "        # im_pil.save(os.path.join(save_path, 'part%d.png' % i))\n",
        "        show.a(['part %d' % i, im_pil.resize((200, 200), Image.ANTIALIAS)], cols=3)\n",
        "    with torch.no_grad():\n",
        "        out = nets.invert(composite, mask_composite)\n",
        "    composite_pil = renormalize.as_image(out[0])\n",
        "    input_np = np.array(renormalize.as_image(composite[0]))\n",
        "    mask_np = np.stack([np.array(mask_composite.cpu()[0][0])] * 3, axis=2)\n",
        "    input_np[mask_np == 0] = 200 # lighten the unfilled region\n",
        "    input_pil = Image.fromarray(input_np)\n",
        "    # input_pil = renormalize.as_image(composite[0])\n",
        "    # composite_pil.save(os.path.join(save_path, 'composite.png'))\n",
        "    show.a(['input', input_pil.resize((200, 200), Image.ANTIALIAS)])\n",
        "    show.a(['composite', composite_pil.resize((200, 200), Image.ANTIALIAS)])\n",
        "    show.flush()\n",
        "\n",
        "show_drawing()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}